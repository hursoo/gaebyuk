import streamlit as st
import pandas as pd
from modules import io_utils, pipeline
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from pathlib import Path
import platform, os
from modules import scraping  # â† ì¶”ê°€


st.set_page_config(page_title="ê°œë²½ í…ìŠ¤íŠ¸ ë¶„ì„ ì›Œí¬í”Œë¡œìš°", page_icon="ğŸ“š", layout="wide")

st.title("ğŸ“š ê°œë²½ í…ìŠ¤íŠ¸ ë¶„ì„ ì›Œí¬í”Œë¡œìš° (ëª¨ë“ˆí˜•)")
st.caption("ë¡œì»¬ íŒŒì¼ ë˜ëŠ” Google Sheetsë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì •ì œ â†’ í† í°í™” â†’ DTM/TF-IDF â†’ í† í”½ëª¨ë¸ë§ â†’ í•´ì„ê¹Œì§€ ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

# ------------------------------------------------------------------------------------
# Sidebar (Step Navigator)
# ------------------------------------------------------------------------------------
steps = [
    "1) ë°ì´í„° ì…ë ¥",
    "2) ì •ì œ/ì „ì²˜ë¦¬",
    "3) í† í°í™”(kiwi/í´ë°±)",
    "4) DTM/TF-IDF",
    "5) í† í”½ëª¨ë¸ë§(LDA/DMR)",
    "6) ê²°ê³¼ íƒìƒ‰/ì‹œê°í™”",
    "7) ê²°ê³¼ ì €ì¥/ë‚´ë³´ë‚´ê¸°"
]
current = st.sidebar.radio("ë‹¨ê³„ ì„ íƒ", steps, index=0)

# Global params on sidebar
st.sidebar.subheader("ğŸ“ ì „ì—­ íŒŒë¼ë¯¸í„°")
ngram_min = st.sidebar.number_input("n-gram ìµœì†Œ", 1, 5, 1)
ngram_max = st.sidebar.number_input("n-gram ìµœëŒ€", 1, 5, 2)
min_df = st.sidebar.number_input("min_df (ë¬¸ì„œ ìµœì†Œ ë“±ì¥ìˆ˜)", 1, 100, 2)
max_features = st.sidebar.number_input("ìµœëŒ€ í”¼ì²˜ ìˆ˜", 100, 100000, 5000, step=500)
use_idf = st.sidebar.checkbox("TF-IDF ì‚¬ìš©", value=True)
remove_one_char = st.sidebar.checkbox("í•œ ê¸€ì í† í° ì œê±°", value=True)

# Session state containers
if "raw_df" not in st.session_state:
    st.session_state.raw_df = None
if "clean_df" not in st.session_state:
    st.session_state.clean_df = None
if "tokens_df" not in st.session_state:
    st.session_state.tokens_df = None
if "dtm" not in st.session_state:
    st.session_state.dtm = None
if "vocab" not in st.session_state:
    st.session_state.vocab = None
if "topic_model" not in st.session_state:
    st.session_state.topic_model = None
if "topic_result" not in st.session_state:
    st.session_state.topic_result = None

# ------------------------------------------------------------------------------------
# Utility: find Korean font
# ------------------------------------------------------------------------------------
def resolve_korean_font():
    local_font = Path(__file__).parent / "assets" / "NanumGothic.ttf"
    if local_font.exists():
        return str(local_font)
    system = platform.system()
    candidates = []
    if system == "Windows":
        candidates += ["C:/Windows/Fonts/malgun.ttf"]
    elif system == "Darwin":
        candidates += ["/System/Library/Fonts/Supplemental/AppleGothic.ttf"]
    else:
        candidates += [
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
            "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        ]
    for p in candidates:
        if os.path.exists(p):
            return p
    return None

font_path = resolve_korean_font()

# ------------------------------------------------------------------------------------
# Step 1: ë°ì´í„° ì…ë ¥
# ------------------------------------------------------------------------------------
if current == steps[0]:
    st.header("1) ë°ì´í„° ì…ë ¥")
    st.write("ì—‘ì…€(.xlsx) ì—…ë¡œë“œ ë˜ëŠ” Google Sheetsì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°(ì„œë¹„ìŠ¤ ê³„ì • í•„ìš”)")

    tab1, tab2, tab3 = st.tabs(["ì—‘ì…€ ì—…ë¡œë“œ", "Google Sheets", "ì›¹ ìŠ¤í¬ë˜í•‘"])

    with tab1:
        up = st.file_uploader("ì—‘ì…€(.xlsx) ì—…ë¡œë“œ", type=["xlsx"])
        sheet_name = st.text_input("ì‹œíŠ¸ ì´ë¦„ (ë¹„ì›Œë‘ë©´ ì²« ì‹œíŠ¸)", "")
        if st.button("ì—‘ì…€ ë¶ˆëŸ¬ì˜¤ê¸°"):
            if up is None:
                st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            else:
                try:
                    df = pd.read_excel(up, sheet_name=sheet_name if sheet_name else 0, engine="openpyxl")
                    st.session_state.raw_df = df
                    st.success(f"ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {df.shape}")
                    st.dataframe(df.head(10))
                except Exception as e:
                    st.error(f"ì—‘ì…€ ì½ê¸° ì‹¤íŒ¨: {e}")

    with tab2:
        gs_url = st.text_input("Google Sheets URL")
        ws_name = st.text_input("ì›Œí¬ì‹œíŠ¸(íƒ­) ì´ë¦„", "")
        st.caption("âš ï¸ Streamlit Cloudì—ì„œ Google Sheetsë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ 'secrets'ì— ì„œë¹„ìŠ¤ ê³„ì • JSONì„ ì €ì¥í•˜ê³ , í•´ë‹¹ ì´ë©”ì¼ì„ ì‹œíŠ¸ ê³µìœ ì— ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.")
        if st.button("ì‹œíŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°"):
            try:
                df = io_utils.read_google_sheet(gs_url, ws_name or None)
                st.session_state.raw_df = df
                st.success(f"ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ: {df.shape}")
                st.dataframe(df.head(10))
            except Exception as e:
                st.error(f"ì‹œíŠ¸ ì½ê¸° ì‹¤íŒ¨: {e}")

    with tab3:
        st.markdown("**ì—‘ì…€/ì‹œíŠ¸ì˜ URL ì—´ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ë°”ë¡œ URL ëª©ë¡ì„ ì…ë ¥í•´ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.**")
        urls_text = st.text_area("URL ëª©ë¡(ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„)", height=150,
                                 placeholder="https://db.history.go.kr/.../rId=XXXXXXXXXXXXXXX\nhttps://db.history.go.kr/.../rId=YYYYYYYYYYYYYYY")
        limit = st.number_input("ì•ì—ì„œë¶€í„° nê°œë§Œ ìˆ˜ì§‘(í…ŒìŠ¤íŠ¸ìš©)", 1, 10000, 10)
        if st.button("ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"):
            urls_list = [u.strip() for u in urls_text.splitlines() if u.strip()]
            if not urls_list:
                st.warning("URLì„ í•œ ì¤„ì— í•˜ë‚˜ì”© ì…ë ¥í•˜ì„¸ìš”.")
            else:
                contents_df = scraping.scrape_contents(urls_list, limit=limit)
                st.success(f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {contents_df.shape}")
                st.dataframe(contents_df.head(10))
                # ë©”íƒ€ì™€ ë³‘í•©ì´ í•„ìš”í•˜ë©´, íƒ­1/íƒ­2ì—ì„œ ì½ì–´ì˜¨ ë©”íƒ€ DF(st.session_state.raw_df)ë¥¼ í™œìš©
                if st.session_state.raw_df is not None:
                    merged = scraping.join_with_meta(
                        st.session_state.raw_df.copy(),
                        contents_df,
                        left_on="r_id_raw", right_on="r_id",
                        keep_cols=["r_id", "r_id_raw", "title", "writer", "gisa_class", "date", "url", "year", "content"]
                    )
                    st.subheader("ë©”íƒ€+ë³¸ë¬¸ ë³‘í•© ê²°ê³¼")
                    st.dataframe(merged.head(10))
                    # ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì“°ë„ë¡ ì €ì¥
                    st.session_state.raw_df = merged
                else:
                    # ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì •ì œ/í† í°í™”ë¡œ ì“°ê²Œ ë§¤í•‘
                    st.session_state.raw_df = contents_df


# ------------------------------------------------------------------------------------
# Step 2: ì •ì œ/ì „ì²˜ë¦¬
# ------------------------------------------------------------------------------------
if current == steps[1]:
    st.header("2) ì •ì œ/ì „ì²˜ë¦¬")
    if st.session_state.raw_df is None:
        st.info("ë¨¼ì € '1) ë°ì´í„° ì…ë ¥'ì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì„¸ìš”.")
    else:
        text_col = st.selectbox("í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆëŠ” ì—´ ì„ íƒ", st.session_state.raw_df.columns.tolist())
        normalize_space = st.checkbox("ê³µë°± ì •ê·œí™”", value=True)
        lower = st.checkbox("ì†Œë¬¸ìí™”(ì˜ë¬¸)", value=False)
        remove_punct = st.checkbox("ê¸°ë³¸ êµ¬ë‘ì  ì œê±°", value=False)

        if st.button("ì •ì œ ì‹¤í–‰"):
            clean_df = pipeline.clean_texts(
                st.session_state.raw_df.copy(),
                text_col=text_col,
                normalize_space=normalize_space,
                to_lower=lower,
                remove_punct=remove_punct,
            )
            st.session_state.clean_df = clean_df
            st.success(f"ì •ì œ ì™„ë£Œ: {clean_df.shape}")
            st.dataframe(clean_df.head(10))

# ------------------------------------------------------------------------------------
# Step 3: í† í°í™”
# ------------------------------------------------------------------------------------
if current == steps[2]:
    st.header("3) í† í°í™”(kiwi ìš°ì„ , soynlp í´ë°±)")
    if st.session_state.clean_df is None:
        st.info("ë¨¼ì € '2) ì •ì œ/ì „ì²˜ë¦¬'ë¥¼ ì™„ë£Œí•˜ì„¸ìš”.")
    else:
        text_col = st.selectbox("í† í°í™”í•  ì—´ ì„ íƒ", st.session_state.clean_df.columns.tolist())
        use_pos = st.checkbox("í’ˆì‚¬ íƒœê¹… ì‚¬ìš©", value=True)
        pos_keep = st.multiselect("ìœ ì§€í•  í’ˆì‚¬", ['NNG', 'NNP', 'VV', 'VA'], default=['NNG', 'NNP'])
        if st.button("í† í°í™” ì‹¤í–‰"):
            tokens_df, used = pipeline.tokenize_texts(
                st.session_state.clean_df.copy(),
                text_col=text_col,
                pos_keep=pos_keep if use_pos else None,
                remove_one_char=remove_one_char,
            )
            st.session_state.tokens_df = tokens_df
            st.success(f"í† í°í™” ì™„ë£Œ (ì—”ì§„: {used})")
            st.dataframe(tokens_df.head(10))

        if st.session_state.tokens_df is not None:
            st.subheader("ìƒìœ„ ë¹ˆë„ í† í°")
            all_tokens = [t for row in st.session_state.tokens_df["tokens"] for t in row]
            top = Counter(all_tokens).most_common(30)
            st.write(top)
            if font_path:
                try:
                    wc = WordCloud(font_path=font_path, background_color="white", width=900, height=500)
                    cloud = wc.generate_from_frequencies(dict(top))
                    fig, ax = plt.subplots()
                    ax.imshow(cloud, interpolation="bilinear")
                    ax.axis("off")
                    st.pyplot(fig)
                except Exception as e:
                    st.warning(f"ì›Œë“œí´ë¼ìš°ë“œ ì‹¤íŒ¨: {e}")

# ------------------------------------------------------------------------------------
# Step 4: DTM/TF-IDF
# ------------------------------------------------------------------------------------
if current == steps[3]:
    st.header("4) DTM/TF-IDF ìƒì„±")
    if st.session_state.tokens_df is None:
        st.info("ë¨¼ì € '3) í† í°í™”'ë¥¼ ì™„ë£Œí•˜ì„¸ìš”.")
    else:
        if st.button("DTM/TF-IDF ë§Œë“¤ê¸°"):
            dtm, vocab, vectorizer = pipeline.build_vector(
                st.session_state.tokens_df,
                ngram_range=(int(ngram_min), int(ngram_max)),
                min_df=int(min_df),
                max_features=int(max_features),
                use_idf=use_idf
            )
            st.session_state.dtm = dtm
            st.session_state.vocab = vocab
            st.success(f"DTM/TF-IDF ì™„ë£Œ: shape={dtm.shape}, vocab={len(vocab)}")
            st.write("ìƒìœ„ í”¼ì²˜:", list(vocab)[:50])

# ------------------------------------------------------------------------------------
# Step 5: í† í”½ëª¨ë¸ë§
# ------------------------------------------------------------------------------------
if current == steps[4]:
    st.header("5) í† í”½ëª¨ë¸ë§ (LDA / DMR ì˜ˆì •)")
    if st.session_state.dtm is None:
        st.info("ë¨¼ì € '4) DTM/TF-IDF'ë¥¼ ì™„ë£Œí•˜ì„¸ìš”.")
    else:
        num_topics = st.number_input("í† í”½ ìˆ˜(k)", 2, 50, 8)
        max_iter = st.number_input("ë°˜ë³µ ìˆ˜", 10, 2000, 500, step=50)
        seed = st.number_input("ì‹œë“œ", 0, 100000, 1000, step=1)
        algo = st.selectbox("ì•Œê³ ë¦¬ì¦˜", ["sklearn-LDA", "tomotopy-LDA (ì‹¤í—˜ì )"], index=0)

        if st.button("í† í”½ í•™ìŠµ"):
            model, result = pipeline.run_topic_model(
                st.session_state.dtm,
                st.session_state.vocab,
                num_topics=int(num_topics),
                max_iter=int(max_iter),
                seed=int(seed),
                algo=algo
            )
            st.session_state.topic_model = model
            st.session_state.topic_result = result
            st.success("í† í”½ í•™ìŠµ ì™„ë£Œ")

# ------------------------------------------------------------------------------------
# Step 6: ê²°ê³¼ íƒìƒ‰
# ------------------------------------------------------------------------------------
if current == steps[5]:
    st.header("6) ê²°ê³¼ íƒìƒ‰/ì‹œê°í™”")
    if st.session_state.topic_result is None:
        st.info("ë¨¼ì € '5) í† í”½ëª¨ë¸ë§'ì„ ì™„ë£Œí•˜ì„¸ìš”.")
    else:
        topn = st.slider("í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ìˆ˜", 5, 30, 10)
        topics = pipeline.get_top_words(st.session_state.topic_result, topn=topn)
        st.subheader("í† í”½ë³„ ìƒìœ„ ë‹¨ì–´")
        st.dataframe(pd.DataFrame({f"Topic {i}": words for i, words in enumerate(topics)}))

# ------------------------------------------------------------------------------------
# Step 7: ê²°ê³¼ ì €ì¥
# ------------------------------------------------------------------------------------
if current == steps[6]:
    st.header("7) ê²°ê³¼ ì €ì¥/ë‚´ë³´ë‚´ê¸°")
    if st.session_state.topic_result is None:
        st.info("ë¨¼ì € '5) í† í”½ëª¨ë¸ë§'ì„ ì™„ë£Œí•˜ì„¸ìš”.")
    else:
        csv = pipeline.export_topics_csv(st.session_state.topic_result)
        st.download_button("í† í”½ ìƒìœ„ì–´ CSV ë‹¤ìš´ë¡œë“œ", data=csv, file_name="topics.csv", mime="text/csv")
